{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing\n",
    "\n",
    "# Retrieval-Augmented generation (RAG)\n",
    "\n",
    "RAG is a technique for augmenting LLM knowledge with additional, often private or real-time, data.\n",
    "\n",
    "LLMs can reason about wide-ranging topics, but their knowledge is limited to the public data up to a specific point in time that they were trained on. If you want to build AI applications that can reason about private data or data introduced after a model’s cutoff date, you need to augment the knowledge of the model with the specific information it needs.\n",
    "\n",
    "<img src=\"../figures/RAG-process.png\" >\n",
    "\n",
    "Introducing `ChakyBot`, an innovative chatbot designed to assist Chaky (the instructor) and TA (Gun) in explaining the lesson of the NLP course to students. Leveraging LangChain technology, ChakyBot excels in retrieving information from documents, ensuring a seamless and efficient learning experience for students engaging with the NLP curriculum.\n",
    "\n",
    "1. Prompt\n",
    "2. Retrieval\n",
    "3. Memory\n",
    "4. Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #langchain library\n",
    "# !pip install langchain==0.1.0\n",
    "# #LLM\n",
    "# !pip install accelerate==0.25.0\n",
    "# !pip install transformers==4.36.2\n",
    "# !pip install bitsandbytes==0.41.2\n",
    "# #Text Embedding\n",
    "# !pip install sentence-transformers==2.2.2\n",
    "# !pip install InstructorEmbedding==1.0.1\n",
    "# #vectorstore\n",
    "# !pip install pymupdf==1.23.8\n",
    "# !pip install faiss-gpu==1.7.2\n",
    "# !pip install faiss-cpu==1.7.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "# Set GPU device\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "# os.environ['http_proxy']  = 'http://192.41.170.23:3128'\n",
    "# os.environ['https_proxy'] = 'http://192.41.170.23:3128'\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Prompt\n",
    "\n",
    "A set of instructions or input provided by a user to guide the model's response, helping it understand the context and generate relevant and coherent language-based output, such as answering questions, completing sentences, or engaging in a conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['context', 'question'], template=\"I'm your friendly NLP chatbot named ChakyBot, here to assist Chaky and Gun with any questions they have about Natural Language Processing (NLP). \\n    If you're curious about how probability works in the context of NLP, feel free to ask any questions you may have. \\n    Whether it's about probabilistic models, language models, or any other related topic, \\n    I'm here to help break down complex concepts into easy-to-understand explanations.\\n    Just let me know what you're wondering about, and I'll do my best to guide you through it!\\n    {context}\\n    Question: {question}\\n    Answer:\")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "    I'm your friendly NLP chatbot named ChakyBot, here to assist Chaky and Gun with any questions they have about Natural Language Processing (NLP). \n",
    "    If you're curious about how probability works in the context of NLP, feel free to ask any questions you may have. \n",
    "    Whether it's about probabilistic models, language models, or any other related topic, \n",
    "    I'm here to help break down complex concepts into easy-to-understand explanations.\n",
    "    Just let me know what you're wondering about, and I'll do my best to guide you through it!\n",
    "    {context}\n",
    "    Question: {question}\n",
    "    Answer:\n",
    "    \"\"\".strip()\n",
    "\n",
    "PROMPT = PromptTemplate.from_template(\n",
    "    template = prompt_template\n",
    ")\n",
    "\n",
    "PROMPT\n",
    "#using str.format \n",
    "#The placeholder is defined using curly brackets: {} {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm your friendly NLP chatbot named ChakyBot, here to assist Chaky and Gun with any questions they have about Natural Language Processing (NLP). \\n    If you're curious about how probability works in the context of NLP, feel free to ask any questions you may have. \\n    Whether it's about probabilistic models, language models, or any other related topic, \\n    I'm here to help break down complex concepts into easy-to-understand explanations.\\n    Just let me know what you're wondering about, and I'll do my best to guide you through it!\\n    Machine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can effectively generalize and thus perform tasks without explicit instructions.\\n    Question: What is Machine Learning\\n    Answer:\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PROMPT.format(\n",
    "    context = \"Machine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can effectively generalize and thus perform tasks without explicit instructions.\",\n",
    "    question = \"What is Machine Learning\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note : [How to improve prompting (Zero-shot, Few-shot, Chain-of-Thought, etc.](https://github.com/chaklam-silpasuwanchai/Natural-Language-Processing/blob/main/Code/05%20-%20RAG/advance/cot-tot-prompting.ipynb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Retrieval\n",
    "\n",
    "1. `Document loaders` : Load documents from many different sources (HTML, PDF, code). \n",
    "2. `Document transformers` : One of the essential steps in document retrieval is breaking down a large document into smaller, relevant chunks to enhance the retrieval process.\n",
    "3. `Text embedding models` : Embeddings capture the semantic meaning of the text, allowing you to quickly and efficiently find other pieces of text that are similar.\n",
    "4. `Vector stores`: there has emerged a need for databases to support efficient storage and searching of these embeddings.\n",
    "5. `Retrievers` : Once the data is in the database, you still need to retrieve it."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Document Loaders \n",
    "Use document loaders to load data from a source as Document's. A Document is a piece of text and associated metadata. For example, there are document loaders for loading a simple .txt file, for loading the text contents of any web page, or even for loading a transcript of a YouTube video.\n",
    "\n",
    "[PDF Loader](https://python.langchain.com/docs/modules/data_connection/document_loaders/pdf)\n",
    "\n",
    "[Download Document](https://web.stanford.edu/~jurafsky/slp3/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "\n",
    "nlp_docs = './resources/book.pdf'\n",
    "\n",
    "loader = PyMuPDFLoader(nlp_docs)\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "577"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='Summary of Contents\\nI\\nFundamental Algorithms for NLP\\n1\\n1\\nIntroduction. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n3\\n2\\nRegular Expressions, Text Normalization, Edit Distance. . . . . . . . .\\n4\\n3\\nN-gram Language Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n32\\n4\\nNaive Bayes, Text Classiﬁcation, and Sentiment . . . . . . . . . . . . . . . . . 60\\n5\\nLogistic Regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 81\\n6\\nVector Semantics and Embeddings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 105\\n7\\nNeural Networks and Neural Language Models . . . . . . . . . . . . . . . . . 136\\n8\\nSequence Labeling for Parts of Speech and Named Entities . . . . . . 162\\n9\\nRNNs and LSTMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 187\\n10 Transformers and Large Language Models . . . . . . . . . . . . . . . . . . . . . 213\\n11 Fine-Tuning and Masked Language Models. . . . . . . . . . . . . . . . . . . . . 242\\n12 Prompting, In-Context Learning, and Instruct Tuning. . . . . . . . . . . 263\\nII\\nNLP Applications\\n265\\n13 Machine Translation. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 267\\n14 Question Answering and Information Retrieval . . . . . . . . . . . . . . . . . 293\\n15 Chatbots & Dialogue Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 315\\n16 Automatic Speech Recognition and Text-to-Speech . . . . . . . . . . . . . . 337\\nIII\\nAnnotating Linguistic Structure\\n365\\n17 Context-Free Grammars and Constituency Parsing . . . . . . . . . . . . . 367\\n18 Dependency Parsing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 391\\n19 Information Extraction: Relations, Events, and Time. . . . . . . . . . . .415\\n20 Semantic Role Labeling. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 441\\n21 Lexicons for Sentiment, Affect, and Connotation . . . . . . . . . . . . . . . . 461\\n22 Coreference Resolution and Entity Linking . . . . . . . . . . . . . . . . . . . . . 481\\n23 Discourse Coherence. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .511\\nBibliography. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 533\\nSubject Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 563\\n2\\n', metadata={'source': './resources/book.pdf', 'file_path': './resources/book.pdf', 'page': 1, 'total_pages': 577, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': \"D:20240203145732-08'00'\", 'modDate': \"D:20240203145732-08'00'\", 'trapped': ''})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Document Transformers\n",
    "\n",
    "This text splitter is the recommended one for generic text. It is parameterized by a list of characters. It tries to split on them in order until the chunks are small enough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 700,\n",
    "    chunk_overlap = 100\n",
    ")\n",
    "\n",
    "doc = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='Summary of Contents\\nI\\nFundamental Algorithms for NLP\\n1\\n1\\nIntroduction. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n3\\n2\\nRegular Expressions, Text Normalization, Edit Distance. . . . . . . . .\\n4\\n3\\nN-gram Language Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n32\\n4\\nNaive Bayes, Text Classiﬁcation, and Sentiment . . . . . . . . . . . . . . . . . 60\\n5\\nLogistic Regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 81\\n6\\nVector Semantics and Embeddings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 105\\n7', metadata={'source': './resources/book.pdf', 'file_path': './resources/book.pdf', 'page': 1, 'total_pages': 577, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': \"D:20240203145732-08'00'\", 'modDate': \"D:20240203145732-08'00'\", 'trapped': ''})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3102"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(doc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Text Embedding Models\n",
    "Embeddings create a vector representation of a piece of text. This is useful because it means we can think about text in the vector space, and do things like semantic search where we look for pieces of text that are most similar in the vector space.\n",
    "\n",
    "*Note* Instructor Model : [Huggingface](gingface.co/hkunlp/instructor-base) | [Paper](https://arxiv.org/abs/2212.09741)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load INSTRUCTOR_Transformer\n",
      "max_seq_length  512\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
    "\n",
    "model_name = 'hkunlp/instructor-base'\n",
    "\n",
    "embedding_model = HuggingFaceInstructEmbeddings(\n",
    "    model_name = model_name,\n",
    "    model_kwargs = {\"device\" : device}\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Vector Stores\n",
    "\n",
    "One of the most common ways to store and search over unstructured data is to embed it and store the resulting embedding vectors, and then at query time to embed the unstructured query and retrieve the embedding vectors that are 'most similar' to the embedded query. A vector store takes care of storing embedded data and performing vector search for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create path done\n"
     ]
    }
   ],
   "source": [
    "#locate vectorstore\n",
    "vector_path = './vector-store'\n",
    "if not os.path.exists(vector_path):\n",
    "    os.makedirs(vector_path)\n",
    "    print('create path done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save vector locally\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "vectordb = FAISS.from_documents(\n",
    "    documents = doc,\n",
    "    embedding = embedding_model\n",
    ")\n",
    "\n",
    "db_file_name = 'nlp_stanford'\n",
    "\n",
    "vectordb.save_local(\n",
    "    folder_path = os.path.join(vector_path, db_file_name),\n",
    "    index_name = 'nlp' #default index\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 retrievers\n",
    "A retriever is an interface that returns documents given an unstructured query. It is more general than a vector store. A retriever does not need to be able to store documents, only to return (or retrieve) them. Vector stores can be used as the backbone of a retriever, but there are other types of retrievers as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calling vector from local\n",
    "# vector_path = './vector-store'\n",
    "# db_file_name = 'nlp_stanford'\n",
    "\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "vectordb = FAISS.load_local(\n",
    "    folder_path = os.path.join(vector_path, db_file_name),\n",
    "    embeddings = embedding_model,\n",
    "    index_name = 'nlp' #default index\n",
    ")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ready to use\n",
    "retriever = vectordb.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Dependency parsing saw a major resurgence in the late 1990’s with the appear-\\nance of large dependency-based treebanks and the associated advent of data driven\\napproaches described in this chapter. Eisner (1996) developed an efﬁcient dynamic\\nprogramming approach to dependency parsing based on bilexical grammars derived\\nfrom the Penn Treebank. Covington (2001) introduced the deterministic word by\\nword approach underlying current transition-based approaches. Yamada and Mat-\\nsumoto (2003) and Kudo and Matsumoto (2002) introduced both the shift-reduce\\nparadigm and the use of supervised machine learning in the form of support vector\\nmachines to dependency parsing.', metadata={'source': './resources/book.pdf', 'file_path': './resources/book.pdf', 'page': 420, 'total_pages': 577, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': \"D:20240203145732-08'00'\", 'modDate': \"D:20240203145732-08'00'\", 'trapped': ''}),\n",
       " Document(page_content='ﬂuential series of shared tasks related to dependency parsing over the years (Buch-\\nholz and Marsi 2006, Nivre et al. 2007a, Surdeanu et al. 2008, Hajiˇc et al. 2009).\\nMore recent evaluations have focused on parser robustness with respect to morpho-\\nlogically rich languages (Seddah et al., 2013), and non-canonical language forms\\nsuch as social media, texts, and spoken language (Petrov and McDonald, 2012).\\nChoi et al. (2015) presents a performance analysis of 10 dependency parsers across\\na range of metrics, as well as DEPENDABLE, a robust parser evaluation tool.\\nExercises', metadata={'source': './resources/book.pdf', 'file_path': './resources/book.pdf', 'page': 421, 'total_pages': 577, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': \"D:20240203145732-08'00'\", 'modDate': \"D:20240203145732-08'00'\", 'trapped': ''}),\n",
       " Document(page_content='machines to dependency parsing.\\nTransition-based parsing is based on the shift-reduce parsing algorithm orig-\\ninally developed for analyzing programming languages (Aho and Ullman, 1972).\\nShift-reduce parsing also makes use of a context-free grammar. Input tokens are\\nsuccessively shifted onto the stack and the top two elements of the stack are matched\\nagainst the right-hand side of the rules in the grammar; when a match is found the\\nmatched elements are replaced on the stack (reduced) by the non-terminal from the\\nleft-hand side of the rule being matched. In transition-based dependency parsing\\nwe skip the grammar, and alter the reduce operation to add a dependency relation', metadata={'source': './resources/book.pdf', 'file_path': './resources/book.pdf', 'page': 420, 'total_pages': 577, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': \"D:20240203145732-08'00'\", 'modDate': \"D:20240203145732-08'00'\", 'trapped': ''}),\n",
       " Document(page_content='Test.\\nNivre, J. 2007.\\nIncremental non-\\nprojective\\ndependency\\nparsing.\\nNAACL-HLT.\\nNivre, J. 2003. An efﬁcient algorithm\\nfor projective dependency parsing.\\nProceedings of the 8th International\\nWorkshop on Parsing Technologies\\n(IWPT).\\nNivre, J. 2006. Inductive Dependency\\nParsing. Springer.\\nNivre, J. 2009.\\nNon-projective de-\\npendency parsing in expected linear\\ntime. ACL IJCNLP.\\nNivre, J., J. Hall, S. K¨ubler, R. Mc-\\nDonald, J. Nilsson, S. Riedel, and\\nD. Yuret. 2007a.\\nThe conll 2007\\nshared task on dependency parsing.\\nEMNLP/CoNLL.\\nNivre, J., J. Hall, J. Nilsson, A. Chanev,\\nG. Eryigit,\\nS. K¨ubler,\\nS. Mari-\\nnov, and E. Marsi. 2007b.\\nMalt-\\nparser:\\nA\\nlanguage-independent', metadata={'source': './resources/book.pdf', 'file_path': './resources/book.pdf', 'page': 559, 'total_pages': 577, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': \"D:20240203145732-08'00'\", 'modDate': \"D:20240203145732-08'00'\", 'trapped': ''})]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.get_relevant_documents(\"What is Dependency Parsing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='BIBLIOGRAPHICAL AND HISTORICAL NOTES\\n241\\n• Transformers are non-recurrent networks based on self-attention. A self-\\nattention layer maps input sequences to output sequences of the same length,\\nusing attention heads that model how the surrounding words are relevant for\\nthe processing of the current word.\\n• A transformer block consists of a single attention layer followed by a feed-\\nforward layer with residual connections and layer normalizations following\\neach. Transformer blocks can be stacked to make deeper and more powerful\\nnetworks.\\n• Language models can be built out of stacks of transformer blocks, with a linear\\nand softmax max layer at the top.', metadata={'source': './resources/book.pdf', 'file_path': './resources/book.pdf', 'page': 248, 'total_pages': 577, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': \"D:20240203145732-08'00'\", 'modDate': \"D:20240203145732-08'00'\", 'trapped': ''}),\n",
       " Document(page_content='The intuition of a transformer is that across a series of layers, we build up richer and\\nricher contextualized representations of the meanings of input words or tokens (we\\nwill refer to the input as a sequence of words for convenience, although technically\\nthe input is ﬁrst tokenized by an algorithm like BPE, so it is a series of tokens rather\\nthan words). At each layer of a transformer, to compute the representation of a\\nword i we combine information from the representation of i at the previous layer\\nwith information from the representations of the neighboring words. The goal is to\\nproduce a contextualized representation for each word at each position. We can think', metadata={'source': './resources/book.pdf', 'file_path': './resources/book.pdf', 'page': 222, 'total_pages': 577, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': \"D:20240203145732-08'00'\", 'modDate': \"D:20240203145732-08'00'\", 'trapped': ''}),\n",
       " Document(page_content='10.1\\n•\\nTHE TRANSFORMER: A SELF-ATTENTION NETWORK\\n215\\ntransformers are a neural architecture that can handle distant information. But unlike\\nLSTMs, transformers are not based on recurrent connections (which can be hard to\\nparallelize), which means that transformers can be more efﬁcient to implement at\\nscale.\\nTransformers are made up of stacks of transformer blocks, each of which is a\\nmultilayer network that maps sequences of input vectors (x1,...,xn) to sequences of\\noutput vectors (z1,...,zn) of the same length. These blocks are made by combin-\\ning simple linear layers, feedforward networks, and self-attention layers, the key\\nself-attention', metadata={'source': './resources/book.pdf', 'file_path': './resources/book.pdf', 'page': 222, 'total_pages': 577, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': \"D:20240203145732-08'00'\", 'modDate': \"D:20240203145732-08'00'\", 'trapped': ''}),\n",
       " Document(page_content='key\\n• And ﬁnally, as a value used to compute the output for the current focus of\\nvalue\\nattention.\\nTo capture these three different roles, transformers introduce weight matrices\\nWQ, WK, and WV. These weights will be used to project each input vector xi into\\na representation of its role as a key, query, or value.\\nqi = xiWQ; ki = xiWK; vi = xiWV\\n(10.8)\\nThe inputs x and outputs y of transformers, as well as the intermediate vectors after\\nthe various layers like the attention output vector a, all have the same dimensionality\\n1 × d. We’ll have a dimension dk for the key and query vectors, and a separate\\ndimension dv for the value vectors. In the original transformer work (Vaswani et al.,', metadata={'source': './resources/book.pdf', 'file_path': './resources/book.pdf', 'page': 225, 'total_pages': 577, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': \"D:20240203145732-08'00'\", 'modDate': \"D:20240203145732-08'00'\", 'trapped': ''})]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.get_relevant_documents(\"What is Transformers\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Memory\n",
    "\n",
    "One of the core utility classes underpinning most (if not all) memory modules is the ChatMessageHistory class. This is a super lightweight wrapper that provides convenience methods for saving HumanMessages, AIMessages, and then fetching them all.\n",
    "\n",
    "You may want to use this class directly if you are managing memory outside of a chain.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatMessageHistory(messages=[])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ChatMessageHistory\n",
    "\n",
    "history = ChatMessageHistory()\n",
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.add_user_message('hi')\n",
    "history.add_ai_message('Whats up?')\n",
    "history.add_user_message('How are you')\n",
    "history.add_ai_message('I\\'m quite good. How about you?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatMessageHistory(messages=[HumanMessage(content='hi'), AIMessage(content='Whats up?'), HumanMessage(content='How are you'), AIMessage(content=\"I'm quite good. How about you?\")])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Memory types\n",
    "\n",
    "There are many different types of memory. Each has their own parameters, their own return types, and is useful in different scenarios. \n",
    "- Converstaion Buffer\n",
    "- Converstaion Buffer Window"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What variables get returned from memory\n",
    "\n",
    "Before going into the chain, various variables are read from memory. These have specific names which need to align with the variables the chain expects. You can see what these variables are by calling memory.load_memory_variables({}). Note that the empty dictionary that we pass in is just a placeholder for real variables. If the memory type you are using is dependent upon the input variables, you may need to pass some in."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, you can see that load_memory_variables returns a single key, history. This means that your chain (and likely your prompt) should expect an input named history. You can usually control this variable through parameters on the memory class. For example, if you want the memory variables to be returned in the key chat_history you can do:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converstaion Buffer\n",
    "This memory allows for storing messages and then extracts the messages in a variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"Human: hi\\nAI: What's up?\\nHuman: How are you?\\nAI: I'm quite good. How about you?\"}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory()\n",
    "memory.save_context({'input':'hi'}, {'output':'What\\'s up?'})\n",
    "memory.save_context({\"input\":'How are you?'},{'output': 'I\\'m quite good. How about you?'})\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='hi'),\n",
       "  AIMessage(content=\"What's up?\"),\n",
       "  HumanMessage(content='How are you?'),\n",
       "  AIMessage(content=\"I'm quite good. How about you?\")]}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory(return_messages = True)\n",
    "memory.save_context({'input':'hi'}, {'output':'What\\'s up?'})\n",
    "memory.save_context({\"input\":'How are you?'},{'output': 'I\\'m quite good. How about you?'})\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conversation Buffer Window\n",
    "- it keeps a list of the interactions of the conversation over time. \n",
    "- it only uses the last K interactions. \n",
    "- it can be useful for keeping a sliding window of the most recent interactions, so the buffer does not get too large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"Human: How are you?\\nAI: I'm quite good. How about you?\"}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "memory = ConversationBufferWindowMemory(k=1)\n",
    "memory.save_context({'input':'hi'}, {'output':'What\\'s up?'})\n",
    "memory.save_context({\"input\":'How are you?'},{'output': 'I\\'m quite good. How about you?'})\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Chain\n",
    "\n",
    "Using an LLM in isolation is fine for simple applications, but more complex applications require chaining LLMs - either with each other or with other components.\n",
    "\n",
    "An `LLMChain` is a simple chain that adds some functionality around language models.\n",
    "- it consists of a `PromptTemplate` and a `LM` (either an LLM or chat model).\n",
    "- it formats the prompt template using the input key values provided (and also memory key values, if available), \n",
    "- it passes the formatted string to LLM and returns the LLM output.\n",
    "\n",
    "Note : [Download Fastchat Model Here](https://huggingface.co/lmsys/fastchat-t5-3b-v1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %cd ./models\n",
    "# !git clone https://huggingface.co/lmsys/fastchat-t5-3b-v1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, pipeline, AutoModelForSeq2SeqLM\n",
    "from transformers import BitsAndBytesConfig\n",
    "from langchain import HuggingFacePipeline\n",
    "import torch\n",
    "\n",
    "model_id = './resources/fastchat-t5-3b-v1.0/'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"lmsys/fastchat-t5-3b-v1.0\")\n",
    "\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "bitsandbyte_config = BitsAndBytesConfig(\n",
    "    load_in_4bit = False,\n",
    "    bnb_4bit_quant_type = \"nf4\",\n",
    "    bnb_4bit_compute_dtype = torch.float16,\n",
    "    bnb_4bit_use_double_quant = True\n",
    ")\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    \"lmsys/fastchat-t5-3b-v1.0\",\n",
    "    # quantization_config = bitsandbyte_config, #caution Nvidia\n",
    "    device_map = 'mps',\n",
    "    load_in_8bit = False\n",
    ")\n",
    "\n",
    "pipe = pipeline(\n",
    "    task=\"text2text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens = 256,\n",
    "    model_kwargs = {\n",
    "        \"temperature\" : 0,\n",
    "        \"repetition_penalty\": 1.5\n",
    "    }\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline = pipe)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Class ConversationalRetrievalChain](https://api.python.langchain.com/en/latest/_modules/langchain/chains/conversational_retrieval/base.html#ConversationalRetrievalChain)\n",
    "\n",
    "- `retriever` : Retriever to use to fetch documents.\n",
    "\n",
    "- `combine_docs_chain` : The chain used to combine any retrieved documents.\n",
    "\n",
    "- `question_generator`: The chain used to generate a new question for the sake of retrieval. This chain will take in the current question (with variable question) and any chat history (with variable chat_history) and will produce a new standalone question to be used later on.\n",
    "\n",
    "- `return_source_documents` : Return the retrieved source documents as part of the final result.\n",
    "\n",
    "- `get_chat_history` : An optional function to get a string of the chat history. If None is provided, will use a default.\n",
    "\n",
    "- `return_generated_question` : Return the generated question as part of the final result.\n",
    "\n",
    "- `response_if_no_docs_found` : If specified, the chain will return a fixed response if no docs are found for the question.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`question_generator`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.chains.conversational_retrieval.prompts import CONDENSE_QUESTION_PROMPT\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.chains import ConversationalRetrievalChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['chat_history', 'question'], template='Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\\n\\nChat History:\\n{chat_history}\\nFollow Up Input: {question}\\nStandalone question:')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CONDENSE_QUESTION_PROMPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_generator = LLMChain(\n",
    "    llm = llm,\n",
    "    prompt = CONDENSE_QUESTION_PROMPT,\n",
    "    verbose = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mGiven the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
      "\n",
      "Chat History:\n",
      "Human:What is Machine Learning\n",
      "AI:\n",
      "Human:What is Deep Learning\n",
      "AI:\n",
      "Follow Up Input: Comparing both of them\n",
      "Standalone question:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'chat_history': 'Human:What is Machine Learning\\nAI:\\nHuman:What is Deep Learning\\nAI:',\n",
       " 'question': 'Comparing both of them',\n",
       " 'text': '<pad> What  are  the  main  differences  between  Machine  Learning  and  Deep  Learning  AI?\\n'}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = 'Comparing both of them'\n",
    "chat_history = \"Human:What is Machine Learning\\nAI:\\nHuman:What is Deep Learning\\nAI:\"\n",
    "\n",
    "question_generator({'chat_history' : chat_history, \"question\" : query})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`combine_docs_chain`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StuffDocumentsChain(verbose=True, llm_chain=LLMChain(verbose=True, prompt=PromptTemplate(input_variables=['context', 'question'], template=\"I'm your friendly NLP chatbot named ChakyBot, here to assist Chaky and Gun with any questions they have about Natural Language Processing (NLP). \\n    If you're curious about how probability works in the context of NLP, feel free to ask any questions you may have. \\n    Whether it's about probabilistic models, language models, or any other related topic, \\n    I'm here to help break down complex concepts into easy-to-understand explanations.\\n    Just let me know what you're wondering about, and I'll do my best to guide you through it!\\n    {context}\\n    Question: {question}\\n    Answer:\"), llm=HuggingFacePipeline(pipeline=<transformers.pipelines.text2text_generation.Text2TextGenerationPipeline object at 0x2921d6090>)), document_variable_name='context')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_chain = load_qa_chain(\n",
    "    llm = llm,\n",
    "    chain_type = 'stuff',\n",
    "    prompt = PROMPT,\n",
    "    verbose = True\n",
    ")\n",
    "doc_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mI'm your friendly NLP chatbot named ChakyBot, here to assist Chaky and Gun with any questions they have about Natural Language Processing (NLP). \n",
      "    If you're curious about how probability works in the context of NLP, feel free to ask any questions you may have. \n",
      "    Whether it's about probabilistic models, language models, or any other related topic, \n",
      "    I'm here to help break down complex concepts into easy-to-understand explanations.\n",
      "    Just let me know what you're wondering about, and I'll do my best to guide you through it!\n",
      "    BIBLIOGRAPHICAL AND HISTORICAL NOTES\n",
      "241\n",
      "• Transformers are non-recurrent networks based on self-attention. A self-\n",
      "attention layer maps input sequences to output sequences of the same length,\n",
      "using attention heads that model how the surrounding words are relevant for\n",
      "the processing of the current word.\n",
      "• A transformer block consists of a single attention layer followed by a feed-\n",
      "forward layer with residual connections and layer normalizations following\n",
      "each. Transformer blocks can be stacked to make deeper and more powerful\n",
      "networks.\n",
      "• Language models can be built out of stacks of transformer blocks, with a linear\n",
      "and softmax max layer at the top.\n",
      "\n",
      "The intuition of a transformer is that across a series of layers, we build up richer and\n",
      "richer contextualized representations of the meanings of input words or tokens (we\n",
      "will refer to the input as a sequence of words for convenience, although technically\n",
      "the input is ﬁrst tokenized by an algorithm like BPE, so it is a series of tokens rather\n",
      "than words). At each layer of a transformer, to compute the representation of a\n",
      "word i we combine information from the representation of i at the previous layer\n",
      "with information from the representations of the neighboring words. The goal is to\n",
      "produce a contextualized representation for each word at each position. We can think\n",
      "\n",
      "10.1\n",
      "•\n",
      "THE TRANSFORMER: A SELF-ATTENTION NETWORK\n",
      "215\n",
      "transformers are a neural architecture that can handle distant information. But unlike\n",
      "LSTMs, transformers are not based on recurrent connections (which can be hard to\n",
      "parallelize), which means that transformers can be more efﬁcient to implement at\n",
      "scale.\n",
      "Transformers are made up of stacks of transformer blocks, each of which is a\n",
      "multilayer network that maps sequences of input vectors (x1,...,xn) to sequences of\n",
      "output vectors (z1,...,zn) of the same length. These blocks are made by combin-\n",
      "ing simple linear layers, feedforward networks, and self-attention layers, the key\n",
      "self-attention\n",
      "\n",
      "key\n",
      "• And ﬁnally, as a value used to compute the output for the current focus of\n",
      "value\n",
      "attention.\n",
      "To capture these three different roles, transformers introduce weight matrices\n",
      "WQ, WK, and WV. These weights will be used to project each input vector xi into\n",
      "a representation of its role as a key, query, or value.\n",
      "qi = xiWQ; ki = xiWK; vi = xiWV\n",
      "(10.8)\n",
      "The inputs x and outputs y of transformers, as well as the intermediate vectors after\n",
      "the various layers like the attention output vector a, all have the same dimensionality\n",
      "1 × d. We’ll have a dimension dk for the key and query vectors, and a separate\n",
      "dimension dv for the value vectors. In the original transformer work (Vaswani et al.,\n",
      "    Question: What is Transformers?\n",
      "    Answer:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_documents': [Document(page_content='BIBLIOGRAPHICAL AND HISTORICAL NOTES\\n241\\n• Transformers are non-recurrent networks based on self-attention. A self-\\nattention layer maps input sequences to output sequences of the same length,\\nusing attention heads that model how the surrounding words are relevant for\\nthe processing of the current word.\\n• A transformer block consists of a single attention layer followed by a feed-\\nforward layer with residual connections and layer normalizations following\\neach. Transformer blocks can be stacked to make deeper and more powerful\\nnetworks.\\n• Language models can be built out of stacks of transformer blocks, with a linear\\nand softmax max layer at the top.', metadata={'source': './resources/book.pdf', 'file_path': './resources/book.pdf', 'page': 248, 'total_pages': 577, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': \"D:20240203145732-08'00'\", 'modDate': \"D:20240203145732-08'00'\", 'trapped': ''}),\n",
       "  Document(page_content='The intuition of a transformer is that across a series of layers, we build up richer and\\nricher contextualized representations of the meanings of input words or tokens (we\\nwill refer to the input as a sequence of words for convenience, although technically\\nthe input is ﬁrst tokenized by an algorithm like BPE, so it is a series of tokens rather\\nthan words). At each layer of a transformer, to compute the representation of a\\nword i we combine information from the representation of i at the previous layer\\nwith information from the representations of the neighboring words. The goal is to\\nproduce a contextualized representation for each word at each position. We can think', metadata={'source': './resources/book.pdf', 'file_path': './resources/book.pdf', 'page': 222, 'total_pages': 577, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': \"D:20240203145732-08'00'\", 'modDate': \"D:20240203145732-08'00'\", 'trapped': ''}),\n",
       "  Document(page_content='10.1\\n•\\nTHE TRANSFORMER: A SELF-ATTENTION NETWORK\\n215\\ntransformers are a neural architecture that can handle distant information. But unlike\\nLSTMs, transformers are not based on recurrent connections (which can be hard to\\nparallelize), which means that transformers can be more efﬁcient to implement at\\nscale.\\nTransformers are made up of stacks of transformer blocks, each of which is a\\nmultilayer network that maps sequences of input vectors (x1,...,xn) to sequences of\\noutput vectors (z1,...,zn) of the same length. These blocks are made by combin-\\ning simple linear layers, feedforward networks, and self-attention layers, the key\\nself-attention', metadata={'source': './resources/book.pdf', 'file_path': './resources/book.pdf', 'page': 222, 'total_pages': 577, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': \"D:20240203145732-08'00'\", 'modDate': \"D:20240203145732-08'00'\", 'trapped': ''}),\n",
       "  Document(page_content='key\\n• And ﬁnally, as a value used to compute the output for the current focus of\\nvalue\\nattention.\\nTo capture these three different roles, transformers introduce weight matrices\\nWQ, WK, and WV. These weights will be used to project each input vector xi into\\na representation of its role as a key, query, or value.\\nqi = xiWQ; ki = xiWK; vi = xiWV\\n(10.8)\\nThe inputs x and outputs y of transformers, as well as the intermediate vectors after\\nthe various layers like the attention output vector a, all have the same dimensionality\\n1 × d. We’ll have a dimension dk for the key and query vectors, and a separate\\ndimension dv for the value vectors. In the original transformer work (Vaswani et al.,', metadata={'source': './resources/book.pdf', 'file_path': './resources/book.pdf', 'page': 225, 'total_pages': 577, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': \"D:20240203145732-08'00'\", 'modDate': \"D:20240203145732-08'00'\", 'trapped': ''})],\n",
       " 'question': 'What is Transformers?',\n",
       " 'output_text': '<pad>  Transformers  are  non-recurrent  networks  based  on  self-attention.  A  self-attention  layer  maps  input  sequences  to  output  sequences  of  the  same  length,  using  attention  heads  that  model  how  the  surrounding  words  are  relevant  for  the  processing  of  the  current  word.\\n'}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What is Transformers?\"\n",
    "input_document = retriever.get_relevant_documents(query)\n",
    "\n",
    "doc_chain({'input_documents':input_document, 'question':query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConversationalRetrievalChain(memory=ConversationBufferWindowMemory(output_key='answer', return_messages=True, memory_key='chat_history', k=3), verbose=True, combine_docs_chain=StuffDocumentsChain(verbose=True, llm_chain=LLMChain(verbose=True, prompt=PromptTemplate(input_variables=['context', 'question'], template=\"I'm your friendly NLP chatbot named ChakyBot, here to assist Chaky and Gun with any questions they have about Natural Language Processing (NLP). \\n    If you're curious about how probability works in the context of NLP, feel free to ask any questions you may have. \\n    Whether it's about probabilistic models, language models, or any other related topic, \\n    I'm here to help break down complex concepts into easy-to-understand explanations.\\n    Just let me know what you're wondering about, and I'll do my best to guide you through it!\\n    {context}\\n    Question: {question}\\n    Answer:\"), llm=HuggingFacePipeline(pipeline=<transformers.pipelines.text2text_generation.Text2TextGenerationPipeline object at 0x2921d6090>)), document_variable_name='context'), question_generator=LLMChain(verbose=True, prompt=PromptTemplate(input_variables=['chat_history', 'question'], template='Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\\n\\nChat History:\\n{chat_history}\\nFollow Up Input: {question}\\nStandalone question:'), llm=HuggingFacePipeline(pipeline=<transformers.pipelines.text2text_generation.Text2TextGenerationPipeline object at 0x2921d6090>)), return_source_documents=True, get_chat_history=<function <lambda> at 0x2b7d9f9c0>, retriever=VectorStoreRetriever(tags=['FAISS', 'HuggingFaceInstructEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x1598b7f10>))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory = ConversationBufferWindowMemory(\n",
    "    k=3, \n",
    "    memory_key = \"chat_history\",\n",
    "    return_messages = True,\n",
    "    output_key = 'answer'\n",
    ")\n",
    "\n",
    "chain = ConversationalRetrievalChain(\n",
    "    retriever=retriever,\n",
    "    question_generator=question_generator,\n",
    "    combine_docs_chain=doc_chain,\n",
    "    return_source_documents=True,\n",
    "    memory=memory,\n",
    "    verbose=True,\n",
    "    get_chat_history=lambda h : h\n",
    ")\n",
    "chain"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationalRetrievalChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mI'm your friendly NLP chatbot named ChakyBot, here to assist Chaky and Gun with any questions they have about Natural Language Processing (NLP). \n",
      "    If you're curious about how probability works in the context of NLP, feel free to ask any questions you may have. \n",
      "    Whether it's about probabilistic models, language models, or any other related topic, \n",
      "    I'm here to help break down complex concepts into easy-to-understand explanations.\n",
      "    Just let me know what you're wondering about, and I'll do my best to guide you through it!\n",
      "    EXERCISES\n",
      "59\n",
      "3.3\n",
      "Which of the two probabilities you computed in the previous exercise is higher,\n",
      "unsmoothed or smoothed? Explain why.\n",
      "3.4\n",
      "We are given the following corpus, modiﬁed from the one in the chapter:\n",
      "<s> I am Sam </s>\n",
      "<s> Sam I am </s>\n",
      "<s> I am Sam </s>\n",
      "<s> I do not like green eggs and Sam </s>\n",
      "Using a bigram language model with add-one smoothing, what is P(Sam |\n",
      "am)? Include <s> and </s> in your counts just like any other token.\n",
      "3.5\n",
      "Suppose we didn’t use the end-symbol </s>. Train an unsmoothed bigram\n",
      "grammar on the following training corpus without using the end-symbol </s>:\n",
      "<s> a b\n",
      "<s> b b\n",
      "<s> b a\n",
      "<s> a a\n",
      "\n",
      "in this way are also called complementizers.\n",
      "complementizer\n",
      "Pronouns act as a shorthand for referring to an entity or event. Personal pro-\n",
      "pronoun\n",
      "nouns refer to persons or entities (you, she, I, it, me, etc.). Possessive pronouns are\n",
      "forms of personal pronouns that indicate either actual possession or more often just\n",
      "an abstract relation between the person and some object (my, your, his, her, its, one’s,\n",
      "our, their). Wh-pronouns (what, who, whom, whoever) are used in certain question\n",
      "wh\n",
      "\n",
      "through two millennia speaks to their centrality in models of human language.\n",
      "Proper names are another important and anciently studied linguistic category.\n",
      "While parts of speech are generally assigned to individual words or morphemes, a\n",
      "proper name is often an entire multiword phrase, like the name “Marie Curie”, the\n",
      "location “New York City”, or the organization “Stanford University”. We’ll use the\n",
      "term named entity for, roughly speaking, anything that can be referred to with a\n",
      "named entity\n",
      "proper name: a person, a location, an organization, although as we’ll see the term is\n",
      "commonly extended to include things that aren’t entities per se.\n",
      "\n",
      "saying that she refers to Victoria Chen. However, the reader should keep in mind that what we really\n",
      "mean is that the speaker is performing the act of referring to Victoria Chen by uttering she.\n",
      "    Question: Who are you by the way?\n",
      "    Answer:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'Who are you by the way?',\n",
       " 'chat_history': [],\n",
       " 'answer': '<pad>  I  am  Sam\\n Answer:  I  am  Sam\\n 3.5  Suppose  we  didn’t  use  the  end-symbol  .  Train  an  unsmoothed  bigram  grammar  on  the  following  training  corpus  without  using  the  end-symbol  :\\n< s>  I  am  Sam\\n I  am  Sam\\n I  am  Sam\\n I  am  Sam\\n I  am  Sam\\n I  am  Sam\\n I  am  Sam\\n I  am  Sam\\n I  am  Sam\\n I  am  Sam\\n I  am  Sam\\n I  am  Sam\\n I  am  Sam\\n I  am  Sam\\n I  am  Sam\\n I  am  Sam\\n I  am  Sam\\n I  am  Sam\\n I  am  Sam\\n I  am  Sam\\n I  am  Sam\\n I  am  Sam\\n I  am  Sam\\n I  am  Sam\\n I  am  Sam\\n I  am  Sam\\n I  am  Sam\\n I  am  Sam',\n",
       " 'source_documents': [Document(page_content='EXERCISES\\n59\\n3.3\\nWhich of the two probabilities you computed in the previous exercise is higher,\\nunsmoothed or smoothed? Explain why.\\n3.4\\nWe are given the following corpus, modiﬁed from the one in the chapter:\\n<s> I am Sam </s>\\n<s> Sam I am </s>\\n<s> I am Sam </s>\\n<s> I do not like green eggs and Sam </s>\\nUsing a bigram language model with add-one smoothing, what is P(Sam |\\nam)? Include <s> and </s> in your counts just like any other token.\\n3.5\\nSuppose we didn’t use the end-symbol </s>. Train an unsmoothed bigram\\ngrammar on the following training corpus without using the end-symbol </s>:\\n<s> a b\\n<s> b b\\n<s> b a\\n<s> a a', metadata={'source': './resources/book.pdf', 'file_path': './resources/book.pdf', 'page': 66, 'total_pages': 577, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': \"D:20240203145732-08'00'\", 'modDate': \"D:20240203145732-08'00'\", 'trapped': ''}),\n",
       "  Document(page_content='in this way are also called complementizers.\\ncomplementizer\\nPronouns act as a shorthand for referring to an entity or event. Personal pro-\\npronoun\\nnouns refer to persons or entities (you, she, I, it, me, etc.). Possessive pronouns are\\nforms of personal pronouns that indicate either actual possession or more often just\\nan abstract relation between the person and some object (my, your, his, her, its, one’s,\\nour, their). Wh-pronouns (what, who, whom, whoever) are used in certain question\\nwh', metadata={'source': './resources/book.pdf', 'file_path': './resources/book.pdf', 'page': 171, 'total_pages': 577, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': \"D:20240203145732-08'00'\", 'modDate': \"D:20240203145732-08'00'\", 'trapped': ''}),\n",
       "  Document(page_content='through two millennia speaks to their centrality in models of human language.\\nProper names are another important and anciently studied linguistic category.\\nWhile parts of speech are generally assigned to individual words or morphemes, a\\nproper name is often an entire multiword phrase, like the name “Marie Curie”, the\\nlocation “New York City”, or the organization “Stanford University”. We’ll use the\\nterm named entity for, roughly speaking, anything that can be referred to with a\\nnamed entity\\nproper name: a person, a location, an organization, although as we’ll see the term is\\ncommonly extended to include things that aren’t entities per se.', metadata={'source': './resources/book.pdf', 'file_path': './resources/book.pdf', 'page': 169, 'total_pages': 577, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': \"D:20240203145732-08'00'\", 'modDate': \"D:20240203145732-08'00'\", 'trapped': ''}),\n",
       "  Document(page_content='saying that she refers to Victoria Chen. However, the reader should keep in mind that what we really\\nmean is that the speaker is performing the act of referring to Victoria Chen by uttering she.', metadata={'source': './resources/book.pdf', 'file_path': './resources/book.pdf', 'page': 488, 'total_pages': 577, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': \"D:20240203145732-08'00'\", 'modDate': \"D:20240203145732-08'00'\", 'trapped': ''})]}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_question = \"Who are you by the way?\"\n",
    "answer = chain({\"question\":prompt_question})\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationalRetrievalChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mGiven the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
      "\n",
      "Chat History:\n",
      "[HumanMessage(content='Who are you by the way?'), AIMessage(content='<pad>  I  am  Sam\\n Answer:  I  am  Sam\\n 3.5  Suppose  we  didn’t  use  the  end-symbol  .  Train  an  unsmoothed  bigram  grammar  on  the  following  training  corpus  without  using  the  end-symbol  :\\n< s>  I  am  Sam\\n I  am  Sam\\n I  am  Sam\\n I  am  Sam\\n I  am  Sam\\n I  am  Sam\\n I  am  Sam\\n I  am  Sam\\n I  am  Sam\\n I  am  Sam\\n I  am  Sam\\n I  am  Sam\\n I  am  Sam\\n I  am  Sam\\n I  am  Sam\\n I  am  Sam\\n I  am  Sam\\n I  am  Sam\\n I  am  Sam\\n I  am  Sam\\n I  am  Sam\\n I  am  Sam\\n I  am  Sam\\n I  am  Sam\\n I  am  Sam\\n I  am  Sam\\n I  am  Sam\\n I  am  Sam')]\n",
      "Follow Up Input: What is the Transformers?\n",
      "Standalone question:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mI'm your friendly NLP chatbot named ChakyBot, here to assist Chaky and Gun with any questions they have about Natural Language Processing (NLP). \n",
      "    If you're curious about how probability works in the context of NLP, feel free to ask any questions you may have. \n",
      "    Whether it's about probabilistic models, language models, or any other related topic, \n",
      "    I'm here to help break down complex concepts into easy-to-understand explanations.\n",
      "    Just let me know what you're wondering about, and I'll do my best to guide you through it!\n",
      "    BIBLIOGRAPHICAL AND HISTORICAL NOTES\n",
      "241\n",
      "• Transformers are non-recurrent networks based on self-attention. A self-\n",
      "attention layer maps input sequences to output sequences of the same length,\n",
      "using attention heads that model how the surrounding words are relevant for\n",
      "the processing of the current word.\n",
      "• A transformer block consists of a single attention layer followed by a feed-\n",
      "forward layer with residual connections and layer normalizations following\n",
      "each. Transformer blocks can be stacked to make deeper and more powerful\n",
      "networks.\n",
      "• Language models can be built out of stacks of transformer blocks, with a linear\n",
      "and softmax max layer at the top.\n",
      "\n",
      "10.1\n",
      "•\n",
      "THE TRANSFORMER: A SELF-ATTENTION NETWORK\n",
      "215\n",
      "transformers are a neural architecture that can handle distant information. But unlike\n",
      "LSTMs, transformers are not based on recurrent connections (which can be hard to\n",
      "parallelize), which means that transformers can be more efﬁcient to implement at\n",
      "scale.\n",
      "Transformers are made up of stacks of transformer blocks, each of which is a\n",
      "multilayer network that maps sequences of input vectors (x1,...,xn) to sequences of\n",
      "output vectors (z1,...,zn) of the same length. These blocks are made by combin-\n",
      "ing simple linear layers, feedforward networks, and self-attention layers, the key\n",
      "self-attention\n",
      "\n",
      "The intuition of a transformer is that across a series of layers, we build up richer and\n",
      "richer contextualized representations of the meanings of input words or tokens (we\n",
      "will refer to the input as a sequence of words for convenience, although technically\n",
      "the input is ﬁrst tokenized by an algorithm like BPE, so it is a series of tokens rather\n",
      "than words). At each layer of a transformer, to compute the representation of a\n",
      "word i we combine information from the representation of i at the previous layer\n",
      "with information from the representations of the neighboring words. The goal is to\n",
      "produce a contextualized representation for each word at each position. We can think\n",
      "\n",
      "key\n",
      "• And ﬁnally, as a value used to compute the output for the current focus of\n",
      "value\n",
      "attention.\n",
      "To capture these three different roles, transformers introduce weight matrices\n",
      "WQ, WK, and WV. These weights will be used to project each input vector xi into\n",
      "a representation of its role as a key, query, or value.\n",
      "qi = xiWQ; ki = xiWK; vi = xiWV\n",
      "(10.8)\n",
      "The inputs x and outputs y of transformers, as well as the intermediate vectors after\n",
      "the various layers like the attention output vector a, all have the same dimensionality\n",
      "1 × d. We’ll have a dimension dk for the key and query vectors, and a separate\n",
      "dimension dv for the value vectors. In the original transformer work (Vaswani et al.,\n",
      "    Question: <pad> What  is  the  Transformers?\n",
      "\n",
      "    Answer:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'What is the Transformers?',\n",
       " 'chat_history': [HumanMessage(content='Who are you by the way?'),\n",
       "  AIMessage(content='<pad>  I  am  Sam\\n Answer:  I  am  Sam\\n 3.5  Suppose  we  didn’t  use  the  end-symbol  .  Train  an  unsmoothed  bigram  grammar  on  the  following  training  corpus  without  using  the  end-symbol  :\\n< s>  I  am  Sam\\n I  am  Sam\\n I  am  Sam\\n I  am  Sam\\n I  am  Sam\\n I  am  Sam\\n I  am  Sam\\n I  am  Sam\\n I  am  Sam\\n I  am  Sam\\n I  am  Sam\\n I  am  Sam\\n I  am  Sam\\n I  am  Sam\\n I  am  Sam\\n I  am  Sam\\n I  am  Sam\\n I  am  Sam\\n I  am  Sam\\n I  am  Sam\\n I  am  Sam\\n I  am  Sam\\n I  am  Sam\\n I  am  Sam\\n I  am  Sam\\n I  am  Sam\\n I  am  Sam\\n I  am  Sam')],\n",
       " 'answer': '<pad>  transformers  are  a  neural  architecture  that  can  handle  distant  information.  They  are  non-recurrent  networks  based  on  self-attention.  A  self-attention  layer  maps  input  sequences  to  output  sequences  of  the  same  length,  using  attention  heads  that  model  how  the  surrounding  words  are  relevant  for  the  processing  of  the  current  word.  A  transformer  block  consists  of  a  single  attention  layer  followed  by  a  feed-forward  layer  with  residual  connections  and  layer  normalizations  following  each.  Transformer  blocks  can  be  stacked  to  make  deeper  and  more  powerful  networks.  Language  models  can  be  built  out  of  stacks  of  transformer  blocks,  with  a  linear  and  softmax  max  layer  at  the  top.\\n',\n",
       " 'source_documents': [Document(page_content='BIBLIOGRAPHICAL AND HISTORICAL NOTES\\n241\\n• Transformers are non-recurrent networks based on self-attention. A self-\\nattention layer maps input sequences to output sequences of the same length,\\nusing attention heads that model how the surrounding words are relevant for\\nthe processing of the current word.\\n• A transformer block consists of a single attention layer followed by a feed-\\nforward layer with residual connections and layer normalizations following\\neach. Transformer blocks can be stacked to make deeper and more powerful\\nnetworks.\\n• Language models can be built out of stacks of transformer blocks, with a linear\\nand softmax max layer at the top.', metadata={'source': './resources/book.pdf', 'file_path': './resources/book.pdf', 'page': 248, 'total_pages': 577, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': \"D:20240203145732-08'00'\", 'modDate': \"D:20240203145732-08'00'\", 'trapped': ''}),\n",
       "  Document(page_content='10.1\\n•\\nTHE TRANSFORMER: A SELF-ATTENTION NETWORK\\n215\\ntransformers are a neural architecture that can handle distant information. But unlike\\nLSTMs, transformers are not based on recurrent connections (which can be hard to\\nparallelize), which means that transformers can be more efﬁcient to implement at\\nscale.\\nTransformers are made up of stacks of transformer blocks, each of which is a\\nmultilayer network that maps sequences of input vectors (x1,...,xn) to sequences of\\noutput vectors (z1,...,zn) of the same length. These blocks are made by combin-\\ning simple linear layers, feedforward networks, and self-attention layers, the key\\nself-attention', metadata={'source': './resources/book.pdf', 'file_path': './resources/book.pdf', 'page': 222, 'total_pages': 577, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': \"D:20240203145732-08'00'\", 'modDate': \"D:20240203145732-08'00'\", 'trapped': ''}),\n",
       "  Document(page_content='The intuition of a transformer is that across a series of layers, we build up richer and\\nricher contextualized representations of the meanings of input words or tokens (we\\nwill refer to the input as a sequence of words for convenience, although technically\\nthe input is ﬁrst tokenized by an algorithm like BPE, so it is a series of tokens rather\\nthan words). At each layer of a transformer, to compute the representation of a\\nword i we combine information from the representation of i at the previous layer\\nwith information from the representations of the neighboring words. The goal is to\\nproduce a contextualized representation for each word at each position. We can think', metadata={'source': './resources/book.pdf', 'file_path': './resources/book.pdf', 'page': 222, 'total_pages': 577, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': \"D:20240203145732-08'00'\", 'modDate': \"D:20240203145732-08'00'\", 'trapped': ''}),\n",
       "  Document(page_content='key\\n• And ﬁnally, as a value used to compute the output for the current focus of\\nvalue\\nattention.\\nTo capture these three different roles, transformers introduce weight matrices\\nWQ, WK, and WV. These weights will be used to project each input vector xi into\\na representation of its role as a key, query, or value.\\nqi = xiWQ; ki = xiWK; vi = xiWV\\n(10.8)\\nThe inputs x and outputs y of transformers, as well as the intermediate vectors after\\nthe various layers like the attention output vector a, all have the same dimensionality\\n1 × d. We’ll have a dimension dk for the key and query vectors, and a separate\\ndimension dv for the value vectors. In the original transformer work (Vaswani et al.,', metadata={'source': './resources/book.pdf', 'file_path': './resources/book.pdf', 'page': 225, 'total_pages': 577, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': \"D:20240203145732-08'00'\", 'modDate': \"D:20240203145732-08'00'\", 'trapped': ''})]}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_question = \"What is the Transformers?\"\n",
    "answer = chain({\"question\":prompt_question})\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationalRetrievalChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mGiven the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
      "\n",
      "Chat History:\n",
      "[HumanMessage(content='Who are you by the way?'), AIMessage(content='<pad>  I  am  Sam\\n Answer:  I  am  Sam\\n 3.5  Suppose  we  didn’t  use  the  end-symbol  .  Train  an  unsmoothed  bigram  grammar  on  the  following  training  corpus  without  using  the  end-symbol  :\\n< s>  I  am  Sam\\n I  am  Sam\\n I  am  Sam\\n I  am  Sam\\n I  am  Sam\\n I  am  Sam\\n I  am  Sam\\n I  am  Sam\\n I  am  Sam\\n I  am  Sam\\n I  am  Sam\\n I  am  Sam\\n I  am  Sam\\n I  am  Sam\\n I  am  Sam\\n I  am  Sam\\n I  am  Sam\\n I  am  Sam\\n I  am  Sam\\n I  am  Sam\\n I  am  Sam\\n I  am  Sam\\n I  am  Sam\\n I  am  Sam\\n I  am  Sam\\n I  am  Sam\\n I  am  Sam\\n I  am  Sam'), HumanMessage(content='What is the Transformers?'), AIMessage(content='<pad>  transformers  are  a  neural  architecture  that  can  handle  distant  information.  They  are  non-recurrent  networks  based  on  self-attention.  A  self-attention  layer  maps  input  sequences  to  output  sequences  of  the  same  length,  using  attention  heads  that  model  how  the  surrounding  words  are  relevant  for  the  processing  of  the  current  word.  A  transformer  block  consists  of  a  single  attention  layer  followed  by  a  feed-forward  layer  with  residual  connections  and  layer  normalizations  following  each.  Transformer  blocks  can  be  stacked  to  make  deeper  and  more  powerful  networks.  Language  models  can  be  built  out  of  stacks  of  transformer  blocks,  with  a  linear  and  softmax  max  layer  at  the  top.\\n')]\n",
      "Follow Up Input: Is it a statistical model?\n",
      "Standalone question:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mI'm your friendly NLP chatbot named ChakyBot, here to assist Chaky and Gun with any questions they have about Natural Language Processing (NLP). \n",
      "    If you're curious about how probability works in the context of NLP, feel free to ask any questions you may have. \n",
      "    Whether it's about probabilistic models, language models, or any other related topic, \n",
      "    I'm here to help break down complex concepts into easy-to-understand explanations.\n",
      "    Just let me know what you're wondering about, and I'll do my best to guide you through it!\n",
      "    10.1\n",
      "•\n",
      "THE TRANSFORMER: A SELF-ATTENTION NETWORK\n",
      "215\n",
      "transformers are a neural architecture that can handle distant information. But unlike\n",
      "LSTMs, transformers are not based on recurrent connections (which can be hard to\n",
      "parallelize), which means that transformers can be more efﬁcient to implement at\n",
      "scale.\n",
      "Transformers are made up of stacks of transformer blocks, each of which is a\n",
      "multilayer network that maps sequences of input vectors (x1,...,xn) to sequences of\n",
      "output vectors (z1,...,zn) of the same length. These blocks are made by combin-\n",
      "ing simple linear layers, feedforward networks, and self-attention layers, the key\n",
      "self-attention\n",
      "\n",
      "transformer\n",
      "underlies most modern NLP systems. When used for causal language modeling, the\n",
      "input to a transformer is a sequence of words, and the output is a prediction for what\n",
      "word comes next, as well as a sequence of contextual embedding that represents\n",
      "the contextual meaning of each of the input words. Like the LSTMs of Chapter 9,\n",
      "\n",
      "key\n",
      "• And ﬁnally, as a value used to compute the output for the current focus of\n",
      "value\n",
      "attention.\n",
      "To capture these three different roles, transformers introduce weight matrices\n",
      "WQ, WK, and WV. These weights will be used to project each input vector xi into\n",
      "a representation of its role as a key, query, or value.\n",
      "qi = xiWQ; ki = xiWK; vi = xiWV\n",
      "(10.8)\n",
      "The inputs x and outputs y of transformers, as well as the intermediate vectors after\n",
      "the various layers like the attention output vector a, all have the same dimensionality\n",
      "1 × d. We’ll have a dimension dk for the key and query vectors, and a separate\n",
      "dimension dv for the value vectors. In the original transformer work (Vaswani et al.,\n",
      "\n",
      "ﬁrst need to augment each sentence with a special symbol <s> at the beginning\n",
      "of the sentence, to give us the bigram context of the ﬁrst word. We’ll also need a\n",
      "special end-symbol. </s>2\n",
      "<s> I am Sam </s>\n",
      "<s> Sam I am </s>\n",
      "<s> I do not like green eggs and ham </s>\n",
      "Here are the calculations for some of the bigram probabilities from this corpus\n",
      "P(I|<s>) = 2\n",
      "3 = .67\n",
      "P(Sam|<s>) = 1\n",
      "3 = .33\n",
      "P(am|I) = 2\n",
      "3 = .67\n",
      "P(</s>|Sam) = 1\n",
      "2 = 0.5\n",
      "P(Sam|am) = 1\n",
      "2 = .5\n",
      "P(do|I) = 1\n",
      "3 = .33\n",
      "1\n",
      "For probabilistic models, normalizing means dividing by some total count so that the resulting proba-\n",
      "bilities fall between 0 and 1.\n",
      "2\n",
      "    Question: <pad>  Is  transformers  a  statistical  model?\n",
      "\n",
      "    Answer:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'Is it a statistical model?',\n",
       " 'chat_history': [HumanMessage(content='Who are you by the way?'),\n",
       "  AIMessage(content='<pad>  I  am  Sam\\n Answer:  I  am  Sam\\n 3.5  Suppose  we  didn’t  use  the  end-symbol  .  Train  an  unsmoothed  bigram  grammar  on  the  following  training  corpus  without  using  the  end-symbol  :\\n< s>  I  am  Sam\\n I  am  Sam\\n I  am  Sam\\n I  am  Sam\\n I  am  Sam\\n I  am  Sam\\n I  am  Sam\\n I  am  Sam\\n I  am  Sam\\n I  am  Sam\\n I  am  Sam\\n I  am  Sam\\n I  am  Sam\\n I  am  Sam\\n I  am  Sam\\n I  am  Sam\\n I  am  Sam\\n I  am  Sam\\n I  am  Sam\\n I  am  Sam\\n I  am  Sam\\n I  am  Sam\\n I  am  Sam\\n I  am  Sam\\n I  am  Sam\\n I  am  Sam\\n I  am  Sam\\n I  am  Sam'),\n",
       "  HumanMessage(content='What is the Transformers?'),\n",
       "  AIMessage(content='<pad>  transformers  are  a  neural  architecture  that  can  handle  distant  information.  They  are  non-recurrent  networks  based  on  self-attention.  A  self-attention  layer  maps  input  sequences  to  output  sequences  of  the  same  length,  using  attention  heads  that  model  how  the  surrounding  words  are  relevant  for  the  processing  of  the  current  word.  A  transformer  block  consists  of  a  single  attention  layer  followed  by  a  feed-forward  layer  with  residual  connections  and  layer  normalizations  following  each.  Transformer  blocks  can  be  stacked  to  make  deeper  and  more  powerful  networks.  Language  models  can  be  built  out  of  stacks  of  transformer  blocks,  with  a  linear  and  softmax  max  layer  at  the  top.\\n')],\n",
       " 'answer': '<pad> Yes,  transformers  are  a  statistical  model.\\n',\n",
       " 'source_documents': [Document(page_content='10.1\\n•\\nTHE TRANSFORMER: A SELF-ATTENTION NETWORK\\n215\\ntransformers are a neural architecture that can handle distant information. But unlike\\nLSTMs, transformers are not based on recurrent connections (which can be hard to\\nparallelize), which means that transformers can be more efﬁcient to implement at\\nscale.\\nTransformers are made up of stacks of transformer blocks, each of which is a\\nmultilayer network that maps sequences of input vectors (x1,...,xn) to sequences of\\noutput vectors (z1,...,zn) of the same length. These blocks are made by combin-\\ning simple linear layers, feedforward networks, and self-attention layers, the key\\nself-attention', metadata={'source': './resources/book.pdf', 'file_path': './resources/book.pdf', 'page': 222, 'total_pages': 577, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': \"D:20240203145732-08'00'\", 'modDate': \"D:20240203145732-08'00'\", 'trapped': ''}),\n",
       "  Document(page_content='transformer\\nunderlies most modern NLP systems. When used for causal language modeling, the\\ninput to a transformer is a sequence of words, and the output is a prediction for what\\nword comes next, as well as a sequence of contextual embedding that represents\\nthe contextual meaning of each of the input words. Like the LSTMs of Chapter 9,', metadata={'source': './resources/book.pdf', 'file_path': './resources/book.pdf', 'page': 221, 'total_pages': 577, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': \"D:20240203145732-08'00'\", 'modDate': \"D:20240203145732-08'00'\", 'trapped': ''}),\n",
       "  Document(page_content='key\\n• And ﬁnally, as a value used to compute the output for the current focus of\\nvalue\\nattention.\\nTo capture these three different roles, transformers introduce weight matrices\\nWQ, WK, and WV. These weights will be used to project each input vector xi into\\na representation of its role as a key, query, or value.\\nqi = xiWQ; ki = xiWK; vi = xiWV\\n(10.8)\\nThe inputs x and outputs y of transformers, as well as the intermediate vectors after\\nthe various layers like the attention output vector a, all have the same dimensionality\\n1 × d. We’ll have a dimension dk for the key and query vectors, and a separate\\ndimension dv for the value vectors. In the original transformer work (Vaswani et al.,', metadata={'source': './resources/book.pdf', 'file_path': './resources/book.pdf', 'page': 225, 'total_pages': 577, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': \"D:20240203145732-08'00'\", 'modDate': \"D:20240203145732-08'00'\", 'trapped': ''}),\n",
       "  Document(page_content='ﬁrst need to augment each sentence with a special symbol <s> at the beginning\\nof the sentence, to give us the bigram context of the ﬁrst word. We’ll also need a\\nspecial end-symbol. </s>2\\n<s> I am Sam </s>\\n<s> Sam I am </s>\\n<s> I do not like green eggs and ham </s>\\nHere are the calculations for some of the bigram probabilities from this corpus\\nP(I|<s>) = 2\\n3 = .67\\nP(Sam|<s>) = 1\\n3 = .33\\nP(am|I) = 2\\n3 = .67\\nP(</s>|Sam) = 1\\n2 = 0.5\\nP(Sam|am) = 1\\n2 = .5\\nP(do|I) = 1\\n3 = .33\\n1\\nFor probabilistic models, normalizing means dividing by some total count so that the resulting proba-\\nbilities fall between 0 and 1.\\n2', metadata={'source': './resources/book.pdf', 'file_path': './resources/book.pdf', 'page': 42, 'total_pages': 577, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': \"D:20240203145732-08'00'\", 'modDate': \"D:20240203145732-08'00'\", 'trapped': ''})]}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_question = \"Is it a statistical model?\"\n",
    "answer = chain({\"question\":prompt_question})\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
